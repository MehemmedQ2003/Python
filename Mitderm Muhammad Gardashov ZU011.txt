3. What is the core idea behind GloVe embeddings and how do they use co-occurrence statistics? 

The main idea of GloVe word embeddings is to learn how words are related by looking at how often they appear together in a big text. Instead of only looking at nearby words like some other models, GloVe checks how often every word appears with every other word in the whole text. It puts this information into a big table called a co-occurrence matrix.

Then, GloVe uses this table to learn numbers (called vectors) for each word. These numbers help show how similar or different words are. GloVe tries to make the math between word vectors match how often the words appear together. It uses something called the “logarithm” of the number of times the words appear together to help do this.

What makes GloVe special is that it doesn’t just look at how often two words are seen together — it looks at how often compared to other words. This helps GloVe find deeper meaning. For example, the difference between “man” and “woman” can be seen in their word vectors, and the same goes for “Paris” and “France.” So, GloVe doesn’t only learn what words mean, but also how they are connected to each other in a smart, mathematical way




4. Why do classical models like SVM still perform well on small datasets?

Classical models like Support Vector Machines (SVMs) can work very well when we don’t have a lot of data. There are a few reasons why.

First, these models are usually simple and do not have many parts to learn. Because of this, they don’t easily make mistakes by learning too much from small data — this problem is called overfitting. SVMs try to find the best line or boundary that separates different groups of data. They choose the line that gives the biggest space between the groups, which helps the model work better even with little data.

Another helpful thing about SVMs is that they can use something called kernel functions. This helps them deal with data that is not easy to separate. The kernel changes the data into a new space where it's easier to divide into groups. So, the model can understand more complex data without needing a lot of examples.

Also, SVMs have a way to control how much they try to be correct versus how simple their boundary is. This balance helps them stay strong even when there isn’t much training data.

Classical models like SVMs are good for small datasets because they are simple, use smart methods like kernels, and can work well without needing a huge amount of data



5. How does the Maximum Entropy (Logistic Regression) classifier make decisions?

The Maximum Entropy (Logistic Regression) classifier makes decisions by predicting the probability that an input belongs to a certain class. It does this using a formula called the logistic function (also called the sigmoid function).
How it works:

1.The model takes the input features (like numbers or categories), multiplies them by weights, and adds a bias (a number to shift the result).
2.Then it applies the logistic function to turn the result into a probability between 0 and 1.
3.If the probability is greater than or equal to 0.5, the classifier predicts the positive class (e.g., class 1). If it's less than 0.5, it predicts the negative class (e.g., class 0).

Training:
The model learns the best weights and bias by looking at examples during training and adjusting them to make better predictions.

In short, the classifier decides which class an input belongs to by calculating a probability and comparing it to a threshold
